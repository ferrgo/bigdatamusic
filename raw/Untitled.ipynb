{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField,StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from sagemaker_pyspark import classpath_jars\n",
    "from pyspark.sql.functions import create_map, struct\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDED = '/home/rlfo/Documents/pessoal/bigdatamusic/landed/'\n",
    "RAW = '/home/rlfo/Documents/pessoal/bigdatamusic/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x7f60f007b860>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "builder = SparkSession.builder.appName(\"MUSIC SPARK\")\n",
    "builder.config(\n",
    "    \"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "builder.config(\"spark.speculation\", \"false\")\n",
    "builder.config(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "builder.config(\"spark.debug.maxToStringFields\", \"100\")\n",
    "builder.config(\"spark.driver.extraClassPath\", classpath)\n",
    "builder.config(\"spark.driver.memory\", \"1g\")\n",
    "builder.config(\"spark.driver.cores\", \"1\")\n",
    "builder.config(\"spark.executor-memory\", \"20g\")\n",
    "builder.config(\"spark.executor.cores\", \"4\")\n",
    "\n",
    "\n",
    "builder.master(\"local[*]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_wav(file):\n",
    "    dst = file.replace('.mp3',\"\")+\".wav\"\n",
    "    sound = AudioSegment.from_mp3(file)\n",
    "    sound.export(dst, format=\"wav\")\n",
    "    return dst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_feature_music(file):\n",
    "    \n",
    "    songname = file.split('/')[0::-1][0]\n",
    "    y, sr = librosa.load(file, mono=True, duration=30)\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "    spec_cent = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "    rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "    \n",
    "    chroma_stft = np.array2string(chroma_stft, precision=4, separator=',',suppress_small=True)\n",
    "    spec_cent = np.array2string(spec_cent, precision=4, separator=',',suppress_small=True)\n",
    "    spec_bw = np.array2string(spec_bw, precision=4, separator=',',suppress_small=True)\n",
    "    rolloff = np.array2string(rolloff, precision=4, separator=',',suppress_small=True)\n",
    "    zcr = np.array2string(zcr, precision=4, separator=',',suppress_small=True)\n",
    "    \n",
    "    \n",
    "    to_append = f'{songname};{chroma_stft};{spec_cent};{spec_bw};{rolloff};{zcr}'    \n",
    "    return to_append\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_music = [f'{LANDED}{file}' for file in os.listdir(LANDED) if '.mp3' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/rlfo/Documents/pessoal/bigdatamusic/landed/Aux Fox - Ellie Goulding - Flux (Aux Fox Remix).mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/Young Nero - Change (Prod. Trippy T).mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/Young Nero - Beyond (Prod. Scott Storch).mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/wūsh - late nights with you.mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/PRDSEOHNO - OHNO - Lil Mama (prod. Fallen Roses and B Dom).mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/nymano - jazz and rain.mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/HIGH ON MUSIC - Danrell x Småland - Hostage.mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/Flipp Dinero - Leave Me Alone (Prod. by Young Forever x Cast Beats).mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/Cardi B - Money.mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/ZZ - ICY (feat. Thorii).mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/Megan Thee Stallion - Cash Shit feat. DaBaby.mp3',\n",
       " '/home/rlfo/Documents/pessoal/bigdatamusic/landed/Kodak Black - ZEZE (feat. Travis Scott and Offset).mp3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rdd_csv = spark.sparkContext.parallelize(all_music).map(audio_to_wav).map(extract_important_feature_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rdd_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('file_name', StringType(), True),\n",
    "                     StructField('chroma', StringType(), True),\n",
    "                     StructField('spec_cent', StringType(), True),\n",
    "                     StructField('spec_bw', StringType(), True),\n",
    "                     StructField('rolloff', StringType(), True),\n",
    "                     StructField('zcr', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+---------+---------+------+\n",
      "|file_name|chroma|spec_cent|  spec_bw|  rolloff|   zcr|\n",
      "+---------+------+---------+---------+---------+------+\n",
      "|         |0.3115|1243.9466|1475.7774|2472.4184|0.0507|\n",
      "|         |0.3245|1914.2462| 2062.584|4129.1167|0.0695|\n",
      "+---------+------+---------+---------+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_rdd_csv = pipe_rdd_csv.map(lambda x : x.split(\";\"))\n",
    "rdd = spark.createDataFrame(pipe_rdd_csv,schema)\n",
    "rdd.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rlfo/Documents/pessoal/bigdatamusic/raw/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-18-0eed648b293e>\", line 1, in <module>\n",
      "    rdd.write.csv('/home/rlfo/raw',sep=';',mode='overwrite')\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/sql/readwriter.py\", line 885, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o218.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:644)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 13, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/pkgutil.py\", line 412, in get_importer\n",
      "    importer = sys.path_importer_cache[path_item]\n",
      "KeyError: ''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 240, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 60, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 171, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 566, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 929, in subimport\n",
      "    __import__(name)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/__init__.py\", line 12, in <module>\n",
      "    from . import core\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/__init__.py\", line 109, in <module>\n",
      "    from .time_frequency import *  # pylint: disable=wildcard-import\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/time_frequency.py\", line 10, in <module>\n",
      "    from ..util.exceptions import ParameterError\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/__init__.py\", line 68, in <module>\n",
      "    from .files import *  # pylint: disable=wildcard-import\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/files.py\", line 7, in <module>\n",
      "    import pkg_resources\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3241, in <module>\n",
      "    @_call_aside\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3225, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3254, in _initialize_master_working_set\n",
      "    working_set = WorkingSet._build_master()\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 574, in _build_master\n",
      "    ws = cls()\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 567, in __init__\n",
      "    self.add_entry(entry)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 623, in add_entry\n",
      "    for dist in find_distributions(entry, True):\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 1960, in find_distributions\n",
      "    importer = get_importer(path_item)\n",
      "  File \"/usr/lib/python3.6/pkgutil.py\", line 416, in get_importer\n",
      "    importer = path_hook(path_item)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1324, in path_hook_for_FileFinder\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 102, in _path_isdir\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n",
      "\t... 31 more\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/pkgutil.py\", line 412, in get_importer\n",
      "    importer = sys.path_importer_cache[path_item]\n",
      "KeyError: ''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 240, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 60, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 171, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 566, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 929, in subimport\n",
      "    __import__(name)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/__init__.py\", line 12, in <module>\n",
      "    from . import core\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/__init__.py\", line 109, in <module>\n",
      "    from .time_frequency import *  # pylint: disable=wildcard-import\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/time_frequency.py\", line 10, in <module>\n",
      "    from ..util.exceptions import ParameterError\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/__init__.py\", line 68, in <module>\n",
      "    from .files import *  # pylint: disable=wildcard-import\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/files.py\", line 7, in <module>\n",
      "    import pkg_resources\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3241, in <module>\n",
      "    @_call_aside\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3225, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3254, in _initialize_master_working_set\n",
      "    working_set = WorkingSet._build_master()\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 574, in _build_master\n",
      "    ws = cls()\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 567, in __init__\n",
      "    self.add_entry(entry)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 623, in add_entry\n",
      "    for dist in find_distributions(entry, True):\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 1960, in find_distributions\n",
      "    importer = get_importer(path_item)\n",
      "  File \"/usr/lib/python3.6/pkgutil.py\", line 416, in get_importer\n",
      "    importer = path_hook(path_item)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1324, in path_hook_for_FileFinder\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 102, in _path_isdir\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/posixpath.py\", line 376, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o218.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:644)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 13, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/python3.6/pkgutil.py\", line 412, in get_importer\n    importer = sys.path_importer_cache[path_item]\nKeyError: ''\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 240, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 60, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 171, in _read_with_length\n    return self.loads(obj)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 566, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 929, in subimport\n    __import__(name)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/__init__.py\", line 12, in <module>\n    from . import core\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/__init__.py\", line 109, in <module>\n    from .time_frequency import *  # pylint: disable=wildcard-import\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/time_frequency.py\", line 10, in <module>\n    from ..util.exceptions import ParameterError\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/__init__.py\", line 68, in <module>\n    from .files import *  # pylint: disable=wildcard-import\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/files.py\", line 7, in <module>\n    import pkg_resources\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3241, in <module>\n    @_call_aside\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3225, in _call_aside\n    f(*args, **kwargs)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3254, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 574, in _build_master\n    ws = cls()\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 567, in __init__\n    self.add_entry(entry)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 623, in add_entry\n    for dist in find_distributions(entry, True):\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 1960, in find_distributions\n    importer = get_importer(path_item)\n  File \"/usr/lib/python3.6/pkgutil.py\", line 416, in get_importer\n    importer = path_hook(path_item)\n  File \"<frozen importlib._bootstrap_external>\", line 1324, in path_hook_for_FileFinder\n  File \"<frozen importlib._bootstrap_external>\", line 102, in _path_isdir\nFileNotFoundError: [Errno 2] No such file or directory\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n\t... 31 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/python3.6/pkgutil.py\", line 412, in get_importer\n    importer = sys.path_importer_cache[path_item]\nKeyError: ''\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 240, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 60, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 171, in _read_with_length\n    return self.loads(obj)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 566, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 929, in subimport\n    __import__(name)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/__init__.py\", line 12, in <module>\n    from . import core\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/__init__.py\", line 109, in <module>\n    from .time_frequency import *  # pylint: disable=wildcard-import\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/core/time_frequency.py\", line 10, in <module>\n    from ..util.exceptions import ParameterError\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/__init__.py\", line 68, in <module>\n    from .files import *  # pylint: disable=wildcard-import\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/librosa/util/files.py\", line 7, in <module>\n    import pkg_resources\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3241, in <module>\n    @_call_aside\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3225, in _call_aside\n    f(*args, **kwargs)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 3254, in _initialize_master_working_set\n    working_set = WorkingSet._build_master()\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 574, in _build_master\n    ws = cls()\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 567, in __init__\n    self.add_entry(entry)\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 623, in add_entry\n    for dist in find_distributions(entry, True):\n  File \"/home/rlfo/.local/share/virtualenvs/bigdatamusic-K9Mh5qpQ/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 1960, in find_distributions\n    importer = get_importer(path_item)\n  File \"/usr/lib/python3.6/pkgutil.py\", line 416, in get_importer\n    importer = path_hook(path_item)\n  File \"<frozen importlib._bootstrap_external>\", line 1324, in path_hook_for_FileFinder\n  File \"<frozen importlib._bootstrap_external>\", line 102, in _path_isdir\nFileNotFoundError: [Errno 2] No such file or directory\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "rdd.write.csv('/home/rlfo/raw',sep=';',mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
